# Task 06 — Sales Trend Analysis Using SQL Aggregations

<p align="left">
    <img src="https://img.shields.io/badge/SQL-Querying-4A4A4A?style=for-the-badge" />
    <img src="https://img.shields.io/badge/Database-PostgreSQL-336791?style=for-the-badge" />
    <img src="https://img.shields.io/badge/Database-MySQL-4479a1?style=for-the-badge" />
    <img src="https://img.shields.io/badge/Database-SQLite-003b57?style=for-the-badge" />
    <img src="https://img.shields.io/badge/Status-Completed-2e7d32?style=for-the-badge" />
</p>

---

## Overview

This task focuses on analyzing **sales trends over time** using SQL aggregation techniques.  
The analysis computes **monthly revenue** and **order volume** from transactional sales data and derives insights from time-based patterns.

The implementation follows **industry-aligned analytics practices**, emphasizing:
- schema-first design
- clear and auditable aggregation logic
- documented assumptions and methodology
- reproducible SQL workflows

---

## Objective

- Analyze **monthly revenue trends**
- Measure **monthly order volume**
- Apply SQL aggregation functions in a production-style manner
- Perform correct time-based grouping using year and month granularity

---

## Dataset Description

**Table Name:** `orders`

| Column Name  | Description                              |
|-------------|------------------------------------------|
| `order_id`  | Unique identifier for each order          |
| `order_date`| Date when the order was placed            |
| `product_id`| Identifier of the purchased product       |
| `amount`    | Total revenue generated by the order      |

The dataset represents a simplified online sales system suitable for aggregation and trend analysis.

---

## Tools & Technologies

- ANSI-compliant SQL
- Supported databases:
  - PostgreSQL
  - MySQL
  - SQLite
- Git & GitHub for version control and collaboration

---

## Project Structure

```
Task-06_Sales-Trend-Analysis/
├── docs/
│   ├── assumptions.md
│   └── methodology.md
├── results/
│   └── monthly_sales_summary.md
├── sql/
│   ├── insights.sql
│   ├── monthly_trends.sql
│   └── schema.sql
└── README.md
```

### Structure Rationale

- **docs/** — documents analytical assumptions and methodology  
- **results/** — presents business-readable findings  
- **sql/** — contains executable SQL only (schema and analysis logic)  

This separation reflects common practices in real-world analytics repositories.

---

## Analysis Performed

### Monthly Revenue
- Aggregated total revenue per month
- Implemented using `SUM(amount)`

### Monthly Order Volume
- Counted unique orders per month
- Implemented using `COUNT(DISTINCT order_id)`

### Time-Based Grouping
- Grouped by **year and month**
- Prevented incorrect aggregation across multiple years

### Business Insight Extension
- Calculated **Average Order Value (AOV)**
- Identified top-performing months by revenue

---

## How to Run the Analysis

### Step 1 — Create the Schema
Execute the schema definition:

```bash
sql/schema.sql
```

### Step 2 — Load Data

Load sales data into the `orders` table using one of the following:

* CSV import
* `INSERT` statements
* Database-specific import tools

### Step 3 — Run Core Aggregation

Execute the monthly aggregation query:

```bash
sql/monthly_trends.sql
```

### Step 4 — Generate Insights

Run the business insight query:

```bash
sql/insights.sql
```

### Step 5 — Review Results

* Review query output tables
* Refer to `results/monthly_sales_summary.md` for a concise business summary

---

## Key Metrics

* **Monthly Revenue**
* **Order Volume**
* **Average Order Value (AOV)**

These metrics are widely used in real-world sales and growth analysis.

---

## Documentation

* [Assumptions](docs/assumptions.md)
* [Methodology](docs/methodology.md)

These documents explain:

* data expectations
* analytical decisions
* known limitations of the analysis

---

## Learning Outcomes

* Practical application of SQL aggregation functions
* Correct handling of time-series data
* Understanding of business-relevant KPIs
* Experience with industry-style project structuring
* Clear separation of logic, documentation, and results

---

## Repository Context

This task is part of a **unified Data Analyst Internship repository**, where all internship tasks are consolidated using industry best practices for:

* repository structure
* documentation quality
* version control workflows

---

## Author

**Athar Shaikh**

Data Analyst Intern  
Elevate Labs  